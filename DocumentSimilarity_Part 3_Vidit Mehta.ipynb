{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plagiarism_lib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ef64eb3e27be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mplagiarism_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mArticleDB\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mArticleDB\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mplagiarism_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjaccard\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mJaccard\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'plagiarism_lib'"
     ]
    }
   ],
   "source": [
    "#################################################################################################\n",
    "# Utilizing Prof. Corada's code (to get to the inputs for Locality Sensitive Hashing functions)\n",
    "#################################################################################################\n",
    "\n",
    "#################################################################################################\n",
    "# Processing the article DB and creating shingles\n",
    "#################################################################################################\n",
    "\n",
    "import string\n",
    "from binascii import crc32\n",
    "\n",
    "def _read_data(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        result = []\n",
    "        for line in f:\n",
    "            first_space = line.find(' ')\n",
    "            article_id = line[:first_space]\n",
    "            text = line[(first_space+1):]\n",
    "            item = (article_id, text)\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "def _process_articles(articles):\n",
    "    _punct_table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "    \n",
    "    def _process_one(x):\n",
    "        docid, text = x\n",
    "        return (docid, text.strip() \\\n",
    "                        .translate(_punct_table) \\\n",
    "                        .lower() \\\n",
    "                        .replace(' ', ''))\n",
    "    \n",
    "    return [_process_one(x) for x in articles]                   \n",
    "\n",
    "def _shingle_document(text, k):\n",
    "    shingles = set()\n",
    "    n = len(text)\n",
    "    for i in range(0, n-k):\n",
    "        shingle = text[i:(i+k)]\n",
    "        hashed_shingle = crc32(shingle.encode('utf-8')) & 0xffffffff\n",
    "        shingles.add(hashed_shingle)\n",
    "    return shingles\n",
    "\n",
    "class ArticleDB:\n",
    "    def __init__(self, filename):\n",
    "        self._filename = filename\n",
    "        self._articles = _read_data(self._filename)\n",
    "        self._processed_articles = _process_articles(self._articles)\n",
    "        self._docids = [docid for docid, _ in self._processed_articles]\n",
    "        \n",
    "    def shingle_data(self, k):\n",
    "        def _shingle_one(x):\n",
    "            docid, text = x\n",
    "            sharded_doc = _shingle_document(text, k)\n",
    "            return (docid, sharded_doc)\n",
    "        \n",
    "        return [_shingle_one(x) for x in self._processed_articles]\n",
    "\n",
    "#################################################################################################\n",
    "# Computing Jaccard Similarity\n",
    "#################################################################################################\n",
    "\n",
    "def _jaccard_similarity(s1, s2):\n",
    "    union_size = len(s1.union(s2))\n",
    "    intersection_size = len(s1.intersection(s2))\n",
    "    return 1. * intersection_size / union_size\n",
    "\n",
    "class Jaccard:\n",
    "    def __init__(self):\n",
    "        self._jaccard_dict = None\n",
    "        \n",
    "    def compute_similarity(self, shingled_data, docids=None):\n",
    "        if docids is not None:\n",
    "            shingled_data = [x for x in shingled_data if x[0] in docids]\n",
    "        \n",
    "        ndocs = len(shingled_data)\n",
    "        self._jaccard_dict = dict()\n",
    "        \n",
    "        for i in range(ndocs-1):\n",
    "            for j in range(i+1, ndocs):\n",
    "                (doci, si) = shingled_data[i]\n",
    "                (docj, sj) = shingled_data[j]\n",
    "                \n",
    "                key = tuple(sorted((doci, docj)))\n",
    "                js = _jaccard_similarity(si, sj)\n",
    "                self._jaccard_dict[key] = js\n",
    "        \n",
    "    def get_similarity(self, doci, docj):\n",
    "        key = tuple(sorted((doci, docj)))\n",
    "        return self._jaccard_dict[key]\n",
    "    \n",
    "#################################################################################################\n",
    "# Part 1 - Main code, calling the functions; Running the experiment which yields k=10\n",
    "#################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from plagiarism_lib.ArticleDB import ArticleDB\n",
    "from plagiarism_lib.jaccard import Jaccard\n",
    "\n",
    "def _setup_fp_dataset(artdb, truth_file):\n",
    "    with open(truth_file, 'r') as f:\n",
    "        truth_pairs = [tuple(sorted(line.strip().split()))\n",
    "                       for line in f]\n",
    "        \n",
    "        true_articles = set()\n",
    "        for s1, s2 in truth_pairs:\n",
    "            true_articles.add(s1)\n",
    "            true_articles.add(s2)\n",
    "        \n",
    "        alldocs = set(artdb._docids)\n",
    "        false_candidates = alldocs.difference(true_articles)\n",
    "        false_partners = random.sample(false_candidates, len(true_articles))\n",
    "        false_pairs = list(zip(true_articles, false_partners))\n",
    "        \n",
    "        alldocs = list(true_articles) + false_partners\n",
    "        return truth_pairs, false_pairs, alldocs\n",
    "\n",
    "def _get_kstats(truth_pairs, false_pairs, jacc):\n",
    "    true_sims = [jacc.get_similarity(x,y) for x,y in truth_pairs]\n",
    "    false_sims = [jacc.get_similarity(x,y) for x,y in false_pairs]\n",
    "    \n",
    "    tp = np.mean(true_sims)\n",
    "    fp = np.mean(false_sims)\n",
    "    \n",
    "    return tp, fp\n",
    "       \n",
    "def run_experiment(train_file, truth_file,\n",
    "                   kvals = [2,5,10,20,40,60,120]):\n",
    "    artdb = ArticleDB(train_file)\n",
    "    truth_pairs, false_pairs, alldocs = _setup_fp_dataset(artdb, truth_file)\n",
    "        \n",
    "    tp = []\n",
    "    fp = []\n",
    "    \n",
    "    for k in kvals:\n",
    "        print(\"Processing data for k=\", k)\n",
    "        shingled_data = artdb.shingle_data(k)\n",
    "        \n",
    "        \n",
    "        jacc = Jaccard()\n",
    "        jacc.compute_similarity(shingled_data, docids=alldocs)\n",
    "        \n",
    "        ktp, kfp = _get_kstats(truth_pairs, false_pairs, jacc)\n",
    "        tp.append(ktp)\n",
    "        fp.append(kfp)\n",
    "     \n",
    "    df = pd.DataFrame({'k': kvals, 'sim_true': tp, 'sim_false': fp})\n",
    "    return df\n",
    "\n",
    "#################################################################################################\n",
    "# Creating Hash functions for Min Hash and creating the vector of hashes for LSH\n",
    "#################################################################################################\n",
    "\n",
    "import random\n",
    "\n",
    "DEFAULT_P = 2**33-355\n",
    "DEFAULT_M = 4294967295\n",
    "\n",
    "def _make_hash(p=DEFAULT_P, m=DEFAULT_M):\n",
    "    a = random.randint(1, p-1)\n",
    "    b = random.randint(0, p-1)\n",
    "    return lambda x: ((a * x + b) % p) % m\n",
    "\n",
    "def _make_hashes(n):\n",
    "    return [_make_hash() for _ in range(n)]\n",
    "\n",
    "\n",
    "def _make_vector_hash(n, m=DEFAULT_M):\n",
    "    hfuncs = _make_hashes(n)\n",
    "    def _f(x):\n",
    "        acc = 0\n",
    "        for i in range(len(x)):\n",
    "            h = hfuncs[i]\n",
    "            acc += h(x[i])\n",
    "        return acc % m\n",
    "    return _f\n",
    "\n",
    "#################################################################################################\n",
    "# Part 2 - Inverting Shingles and Code to create MinHash Signature Matrix\n",
    "#################################################################################################\n",
    "\n",
    "import numpy as np\n",
    "import _make_hashes\n",
    "\n",
    "# Invert shingled article dataset so we can iterate over shingles\n",
    "#\n",
    "# shingled_data: list of (docid, shingles) tuples\n",
    "#    docid: document id \n",
    "#    shingles: set of 32-bit integers encoding document shingles of corresponding document\n",
    "#\n",
    "# returns: sorted list of tuples (s, docid):\n",
    "#    s: 32-bit integer encoding document shingle\n",
    "#    docid: id of document where s occurs\n",
    "def invert_shingles(shingled_data):\n",
    "    inv_index = []\n",
    "    docids = []\n",
    "    \n",
    "    for (docid, shingles) in shingled_data:\n",
    "        docids.append(docid)\n",
    "        for s in shingles:\n",
    "            item = (s, docid)\n",
    "            inv_index.append(item)\n",
    "    return sorted(inv_index), docids\n",
    "\n",
    "\n",
    "# Create a minhash signature matrix\n",
    "#\n",
    "# shingled_data: a shingled article dataset, format depends on argument 'inverted' (see below)\n",
    "# num_hashes: number of hash functions to use in the minhash summary\n",
    "# inverted: boolean, is the shingled data already inverted (see below)\n",
    "#\n",
    "# returns: tuple (mh, docids) \n",
    "#   mh: a numpy matrix (num_hashes x num_docs) of minhash signature\n",
    "#   docids: a list of document ids\n",
    "# \n",
    "# note: if argument 'inverted' is True, then shingled data is already sorted by shingle, so\n",
    "# the minhash algorithm can use directly. In this case, 'shingled_data' is assumed to be \n",
    "# a sorted list of (s, docid) tuples, with s a shingle, and docid a document id/\n",
    "#\n",
    "# If argument 'inverted' is False, then shingled data is organized by document and we need\n",
    "# to create an inverted index so we can iterate by shingle. In this case 'shingled_data' is\n",
    "# assumed to be a list of (docid, shingles) tuples with docid a document id and\n",
    "# shingles a 'set' of shingles\n",
    "def _make_minhash_sigmatrix(shingled_data, num_hashes, inverted=False):\n",
    "    \n",
    "    # invert the shingled data if necessary\n",
    "    if inverted:\n",
    "        inv_index, docids = shingled_data\n",
    "    else:\n",
    "        inv_index, docids = invert_shingles(shingled_data)\n",
    "        \n",
    "    # initialize the signature matrix with infinity in every entry\n",
    "    num_docs = len(docids)\n",
    "    sigmat = np.full([num_hashes, num_docs], np.inf)       # create a matrix = num_hashes x num_docs\n",
    "    \n",
    "    # create num_hashes random hash functions\n",
    "    hash_funcs = _make_hashes(num_hashes)                   # yields a list of 'num_hashes' number of hash functions\n",
    "    \n",
    "    # iterate over shingles \n",
    "    for s, docid in inv_index:\n",
    "        # evaluate hash functions on shingle\n",
    "        hashvals = [h(s) for h in hash_funcs]  # yields a list of hashed values of shingle s, passed to each of hash functions \n",
    "        \n",
    "        # find the appropriate column in signature matrix for document id\n",
    "        j = docids.index(docid)\n",
    "        \n",
    "        # update rows of appropriate column as needed\n",
    "        for i in range(num_hashes):\n",
    "            if hashvals[i] < sigmat[i,j]:      # logic yields the minimum value of shingle s evaluated from all hash functions\n",
    "                # saw a smaller hash value, update the signature matrix\n",
    "                sigmat[i,j] = hashvals[i]\n",
    "    return sigmat, docids\n",
    "\n",
    "\n",
    "# Objects used to create and query minhash summaries\n",
    "# Example using 10 hash functions:\n",
    "#   mh = MinHash(10) \n",
    "#   mh.make_matrix(article_db)\n",
    "#   mh.get_similarity(doci, docj)\n",
    "class MinHash:\n",
    "    # Construct an empty object that will use 'num_hashes' in the summary\n",
    "    #\n",
    "    # num_hashes: number of hashes to use in the \n",
    "    def __init__(self, num_hashes):\n",
    "        self._num_hashes = num_hashes\n",
    "        self._docids = None\n",
    "        self._mat = None\n",
    "        \n",
    "    # Create the signature matrix\n",
    "    # \n",
    "    # shingled_data: a shingled document dataset (see _make_minhash_sigmatrix)\n",
    "    # inverted: is the dataset already inverted (see _make_minhash_sigmatrix)\n",
    "    #\n",
    "    # side effect: updates self._docids and self._mat atttributes of object\n",
    "    def make_matrix(self, shingled_data, inverted=False):\n",
    "        self._mat, self._docids = _make_minhash_sigmatrix(shingled_data,\n",
    "                                                          self._num_hashes,\n",
    "                                                          inverted=inverted)\n",
    "        \n",
    "    # compute minhash JS estimate for documents di and dj\n",
    "    #\n",
    "    # di: id of first document\n",
    "    # dj: id of second document\n",
    "    #\n",
    "    # returns: minhash JS estimate (double)\n",
    "    def get_similarity(self, di, dj):\n",
    "        i = self._docids.index(di)\n",
    "        j = self._docids.index(dj)\n",
    "        return np.mean(self._mat[:,i] == self._mat[:,j])\n",
    "    \n",
    "\n",
    "#################################################################################################\n",
    "# Main Code for Part 2 - Calling Functions\n",
    "#################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from plagiarism_lib.jaccard import Jaccard\n",
    "from plagiarism_lib.minhash import MinHash, invert_shingles\n",
    "\n",
    "## \n",
    "# Create a pandas DataFrame containing exact Jaccard similarities\n",
    "# between pairs of documents\n",
    "#\n",
    "# exp_data: a shingled article dataset\n",
    "# docids: ids for documents in 'exp_data'\n",
    "#\n",
    "# returns: a pandas DataFrame with columns:\n",
    "#   doci: id of first document\n",
    "#   docj: id of second document\n",
    "#   js: Jaccard similarity between pair of documents\n",
    "def make_js_df(exp_data, docids):\n",
    "    # create a Jaccard object and compute pairwise similarities\n",
    "    jacc = Jaccard()\n",
    "    jacc.compute_similarity(exp_data)\n",
    "\n",
    "    # create the data frame with all pairwise Jaccard similarities\n",
    "    ndocs = len(docids)\n",
    "    doci_series = []\n",
    "    docj_series = []\n",
    "    js_series = []\n",
    "    \n",
    "    for i in range(ndocs-1):\n",
    "        for j in range(i+1, ndocs):\n",
    "            di = docids[i]\n",
    "            dj = docids[j]\n",
    "            s = jacc.get_similarity(di, dj)\n",
    "            doci_series.append(di)\n",
    "            docj_series.append(dj)\n",
    "            js_series.append(s)\n",
    "            \n",
    "    return pd.DataFrame({'doci': doci_series, \n",
    "                         'docj': docj_series, \n",
    "                         'js': js_series})\n",
    "\n",
    "\n",
    "##\n",
    "# Run an experiment to measure error of MinHash Jaccard similarity estimates\n",
    "# for various number of hash functions used in MinHash summaries\n",
    "#\n",
    "# exp_data: a shingled article dataset\n",
    "# exp_df: a pandas DataFrame with exact JS computed between pairs of articles \n",
    "# hash_vals: number of hash functions to use in the various MinHash JS estimates\n",
    "#\n",
    "# side effect: adds columns to exp_df:\n",
    "#   mh_<k>: Minhash estimate of JS for pair of documents using k hash functions\n",
    "def run_mh_exp(exp_data, exp_df, hash_vals=[10,20,50,100,1000]):\n",
    "    \n",
    "    # invert the shingled articles so we can iterate over\n",
    "    # shingles \n",
    "    inv_data = invert_shingles(exp_data)\n",
    "    \n",
    "    # for each of the number of hashes used in experiment\n",
    "    for num_hash in hash_vals:\n",
    "        print(\"Doing minhash for \", num_hash, \" hashes\")\n",
    "        \n",
    "        # compute minhahs signature matrix\n",
    "        mh = MinHash(num_hash)\n",
    "        mh.make_matrix(inv_data, inverted=True)\n",
    "        \n",
    "        # add the minhash JS estimates for given number of hash functions\n",
    "        # to experiment dataset\n",
    "        cur_series = []\n",
    "        for _, row in exp_df.iterrows():\n",
    "            s = mh.get_similarity(row['doci'], row['docj'])\n",
    "            cur_series.append(s)\n",
    "            \n",
    "        series_name = 'mh_' + str(num_hash)\n",
    "        exp_df[series_name] = cur_series\n",
    "  \n",
    "# Compute root mean squared error between Minhash estimates and exact JS\n",
    "# \n",
    "# exp_df: pandas DataFrame with exact JS and minhash JS similarities (created by run_mh_exp)\n",
    "# hash_vals: the hash values used in the various minhash estimates\n",
    "#\n",
    "# returns: a pandas DataFrame with columns\n",
    "#    h: hash value used in minhash estimate\n",
    "#    rmse: root mean squared error of minhash estimate\n",
    "def post_process_df(exp_df, hash_vals):\n",
    "    tmp_df = exp_df\n",
    "    for num_hash in hash_vals:\n",
    "        mh_series = 'mh_' + str(num_hash)\n",
    "        series_name = 'diff_' + str(num_hash)\n",
    "        tmp_df[series_name] = np.square(exp_df['js'] - exp_df[mh_series])\n",
    "    \n",
    "    cols = ['diff_' + str(num_hash) for num_hash in hash_vals]\n",
    "    mns = tmp_df[cols].mean()\n",
    "    rmse_df = pd.DataFrame({'h': hash_vals, 'rmse': np.sqrt(mns)})\n",
    "    return rmse_df\n",
    "\n",
    "#################################################################################################\n",
    "# Start of Part 3 - Function to get the number of Bands in LSH algorithm (based on optimization of error function)\n",
    "#################################################################################################\n",
    "\n",
    "import scipy.optimize as opt\n",
    "import math\n",
    "\n",
    "# Choose number of bands for LSH based on threshold t and number of rows\n",
    "# in minhash signature matrix n\n",
    "#\n",
    "# t: desired threshold for candidate detection (double in (0,1))\n",
    "# n: number of rows in minhash signature matrix\n",
    "#\n",
    "# returns: tuple (b, final_t)\n",
    "#    b: number of bands (integer such that b*r = n)\n",
    "#    final_t: the final threshold t = (1/b)^(1/r) s.t. b*r=n\n",
    "#\n",
    "# Uses Nelder-Mead optimization method to find value b such that\n",
    "# (1/b)^(b/n) is closest (in least squares) value possible to input threshold t\n",
    "#\n",
    "# Example:\n",
    "#   b, final_t = _choose_nbands(.8, 100)\n",
    "def _choose_nbands(t, n):\n",
    "    def _error_fun(x):\n",
    "        cur_t = (1/x[0])**(x[0]/n)\n",
    "        return (t-cur_t)**2\n",
    "    \n",
    "    opt_res = opt.minimize(_error_fun, x0=(10), method='Nelder-Mead')\n",
    "    b = int(math.ceil(opt_res['x'][0]))\n",
    "    r = int(n / b)\n",
    "    final_t = (1/b)**(1/r)\n",
    "    return b, final_t\n",
    "\n",
    "#################################################################################################\n",
    "# Implementing LSH\n",
    "#################################################################################################\n",
    "\n",
    "#################################################################################################\n",
    "# Part 1 - Do_LSH function\n",
    "#################################################################################################\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def do_lsh(minhash_sigmatrix, numhashes, docids, threshold):\n",
    "  # choose the number of bands, and rows per band to use in LSH\n",
    "  b, _ = _choose_nbands(threshold, numhashes)\n",
    "  r = int(numhashes / b)          # Number of rows in each band\n",
    "\n",
    "  narticles = len(docids)\n",
    "\n",
    "  # generate a random hash function that takes vectors of length r as input\n",
    "  hash_func = _make_vector_hash(r)\n",
    "\n",
    "  # setup the list of hashtables, will be populated with one hashtable per band\n",
    "  buckets = []\n",
    "\n",
    "  # fill hash tables for each band\n",
    "  for band in range(b):\n",
    "    # figure out which rows of minhash signature matrix to hash for this band\n",
    "    start_index = int(band * r)\n",
    "    end_index = min(start_index + r, numhashes)\n",
    "\n",
    "    # initialize hashtable for this band\n",
    "    cur_buckets = defaultdict(list)\n",
    "    \n",
    "    for j in range(narticles):                          # to access each doc ID from sigmatrix\n",
    "        m = []                                          # List to collect hash values for band 'b' for document 'j'\n",
    "        for k in range(start_index, end_index+1):       # this loop will extract the rows from sigmatrix\n",
    "            m.append(minhash_sigmatrix[k,j])            # Collecting the minhash values from sigmatrix\n",
    "        hashval = hash_func(m)                          # Computing LSH hash value by passing the list of minhash values\n",
    "        cur_buckets[hashval].append(docids[j])          # Capturing the document ids, and indexing it with the hashvalue\n",
    "    \n",
    "    # add this hashtable to the list of hashtables\n",
    "    buckets.append(cur_buckets)\n",
    "\n",
    "  return buckets\n",
    "\n",
    "#################################################################################################\n",
    "# Part 2 - Finding candidate similar article pairs\n",
    "#################################################################################################\n",
    "\n",
    "def find_candidate_pair(buckets):\n",
    "    candidate_pair = []\n",
    "    hashvalues = []\n",
    "    docid = []\n",
    "    for b in buckets:                          # for each bucket, we know similar docs will hash to the same bucket\n",
    "        for hval,id in b.items():              # extracting hash values, list of document IDs\n",
    "            hashvalues.append(hval)\n",
    "            docid.append(id)                   # is a nested list of doc IDs\n",
    "        docid = tuple(docid)\n",
    "        for i in range(len(docid)):\n",
    "            id1 = []\n",
    "            id1.append(docid[i])\n",
    "            for j in range(i+1,len(docid)):\n",
    "                id2 = []\n",
    "                id2.append(docid[j])\n",
    "                tup = tuple(id1) + tuple(id2)\n",
    "                candidate_pair.append(tup)     # creating the list of tuples with candidate pairs of docs\n",
    "    return candidate_pair\n",
    "\n",
    "#################################################################################################\n",
    "# Experiment 2 - Computing Precision and Recall\n",
    "#################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "def experiment2_LSH(candidate_pair, threshold):\n",
    "    mainfile = r'C:\\Vidit\\PhD\\Fall 2018\\CMSC 643 - Hector\\Project 1\\TestFile10000.txt'  #Getting 10000 file\n",
    "    with open(mainfile, 'r') as f:\n",
    "        all_pairs = [tuple(sorted(line.strip().split()))\n",
    "                       for line in f]\n",
    "    total_articles = set()\n",
    "    for s1, s2 in all_pairs:\n",
    "        total_articles.add(s1)\n",
    "        total_articles.add(s2)\n",
    "    truth_file = r'C:\\Vidit\\PhD\\Fall 2018\\CMSC 643 - Hector\\Project 1\\TruthFile10000.txt'  #Getting 10000 truth file\n",
    "    with open(truth_file, 'r') as f:\n",
    "        truth_pairs = [tuple(sorted(line.strip().split()))\n",
    "                       for line in f]\n",
    "    true_articles = set()\n",
    "    for s1, s2 in truth_pairs:\n",
    "        true_articles.add(s1)\n",
    "        true_articles.add(s2)\n",
    "    threshold = input(\"Enter the threshold value for similarity:\")\n",
    "    numhashes = input(\"Enter the no of hash functions:\")\n",
    "    buckets = do_lsh(minhash_sigmatrix, numhashes, docids, threshold)\n",
    "    candidate_pair = find_candidate_pair(buckets)\n",
    "    result = []\n",
    "    true_positive = false_positive = false_negative = true_negative = 0\n",
    "    for id1,id2 in total_articles:                              # accessing pairs of doc ids in 10000 dataset\n",
    "        for i,j in candidate_pair:                              # accessing pairs of candidate docs from LSH function output\n",
    "            tup = ()\n",
    "            for k,l in true_articles:                           # accessing pairs of doc ids in 10000 truth file\n",
    "                if (id1 == k and id2 == l and id1 == i and id2 == j):\n",
    "                    tup = tuple([id1]) + tuple([id2]) + tuple(['Yes']) + tuple(['Yes'])\n",
    "                    true_positive = true_positive + 1\n",
    "                    result.append(tup)\n",
    "                elif (id1 != k and id2 != l and id1 == i and id2 == j):\n",
    "                    tup = tuple([id1]) + tuple([id2]) + tuple(['No']) + tuple(['Yes'])\n",
    "                    false_positive = false_positive + 1\n",
    "                    result.append(tup)\n",
    "                elif (id1 == k and id2 == l and id1 != i and id2 != j):\n",
    "                    tup = tuple([id1]) + tuple([id2]) + tuple(['Yes']) + tuple(['No'])\n",
    "                    false_negative = false_negative + 1\n",
    "                    result.append(tup)\n",
    "                elif (id1 != k and id2 != l and id1 != i and id2 != j):\n",
    "                    tup = tuple([id1]) + tuple([id2]) + tuple(['No']) + tuple(['No'])\n",
    "                    true_negative = true_negative + 1\n",
    "                    result.append(tup)\n",
    "    df = pd.DataFrame(result, columns=['ID1', 'ID2', 'True Label','LSH Label'])\n",
    "    precision = true_positive/(true_positive + false_positive)\n",
    "    recall = true_positive/(true_positive + false_negative)\n",
    "    return precision, recall\n",
    "    \n",
    "                    \n",
    "                    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "                \n",
    "                \n",
    "            \n",
    "                \n",
    "        \n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
